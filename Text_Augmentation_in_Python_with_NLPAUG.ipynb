{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1auQGhaQe3UUzmOfRqKVTTXjaNgGDuL0A",
      "authorship_tag": "ABX9TyPBRVbHpmhzbkybDVGwzpr7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kstyle2198/NLP_TIPS/blob/main/Text_Augmentation_in_Python_with_NLPAUG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://medium.com/@marc.bolle/text-augmentation-in-python-with-nlpaug-48c3eebacf46\n"
      ],
      "metadata": {
        "id": "IMPGfLp_KH0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvP_ruEKJX9o",
        "outputId": "45a2f54f-db13-4ada-95f6-5c49d1a32fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.11.17)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Augmenter"
      ],
      "metadata": {
        "id": "hdT5U5n3KLpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.datasets\n",
        "import re\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.model.word_stats as nmw\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "WbpoD70MJcF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfd6199-7596-48ce-b0f8-633ec6bb8c41"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a tokenizer function to extract word tokens\n",
        "def _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
        "    token_pattern = re.compile(token_pattern)\n",
        "    return token_pattern.findall(text)"
      ],
      "metadata": {
        "id": "z_9D37DkJcC8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sample data (Scikit Learn 20 News Groups)\n",
        "train_data = sklearn.datasets.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "train_x = train_data.data\n",
        "train_x[:1]"
      ],
      "metadata": {
        "id": "-raE7GQsJcAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec683a3-f675-4047-9a4d-30afc4efe4ec"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input\n",
        "train_x_tokens = [_tokenizer(x) for x in train_x]\n",
        "train_x_tokens[:1]"
      ],
      "metadata": {
        "id": "GMGhWcnQJb9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5bc4ef-3d52-473d-9ae1-7d5b60954a1c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['was',\n",
              "  'wondering',\n",
              "  'if',\n",
              "  'anyone',\n",
              "  'out',\n",
              "  'there',\n",
              "  'could',\n",
              "  'enlighten',\n",
              "  'me',\n",
              "  'on',\n",
              "  'this',\n",
              "  'car',\n",
              "  'saw',\n",
              "  'the',\n",
              "  'other',\n",
              "  'day',\n",
              "  'It',\n",
              "  'was',\n",
              "  'door',\n",
              "  'sports',\n",
              "  'car',\n",
              "  'looked',\n",
              "  'to',\n",
              "  'be',\n",
              "  'from',\n",
              "  'the',\n",
              "  'late',\n",
              "  '60s',\n",
              "  'early',\n",
              "  '70s',\n",
              "  'It',\n",
              "  'was',\n",
              "  'called',\n",
              "  'Bricklin',\n",
              "  'The',\n",
              "  'doors',\n",
              "  'were',\n",
              "  'really',\n",
              "  'small',\n",
              "  'In',\n",
              "  'addition',\n",
              "  'the',\n",
              "  'front',\n",
              "  'bumper',\n",
              "  'was',\n",
              "  'separate',\n",
              "  'from',\n",
              "  'the',\n",
              "  'rest',\n",
              "  'of',\n",
              "  'the',\n",
              "  'body',\n",
              "  'This',\n",
              "  'is',\n",
              "  'all',\n",
              "  'know',\n",
              "  'If',\n",
              "  'anyone',\n",
              "  'can',\n",
              "  'tellme',\n",
              "  'model',\n",
              "  'name',\n",
              "  'engine',\n",
              "  'specs',\n",
              "  'years',\n",
              "  'of',\n",
              "  'production',\n",
              "  'where',\n",
              "  'this',\n",
              "  'car',\n",
              "  'is',\n",
              "  'made',\n",
              "  'history',\n",
              "  'or',\n",
              "  'whatever',\n",
              "  'info',\n",
              "  'you',\n",
              "  'have',\n",
              "  'on',\n",
              "  'this',\n",
              "  'funky',\n",
              "  'looking',\n",
              "  'car',\n",
              "  'please',\n",
              "  'mail']]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train TF-IDF model\n",
        "tfidf_model = nmw.TfIdf()\n",
        "tfidf_model.train(train_x_tokens)\n",
        "# tfidf_model.save('.')\n",
        "tfidf_model"
      ],
      "metadata": {
        "id": "yuduj0JXJb6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10d6fbd-9623-4aa8-a053-14e270548128"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nlpaug.model.word_stats.tfidf.TfIdf at 0x7afceede37f0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"It was a dark and stormy night. I was alone at home when I saw a lion's face followed by a scary thunderous roar at the windows.\"\n",
        "# text = \"I like eating an apple\"\n",
        "text = \"light signal column on the upper deck does not work well and to be replaced with new one\""
      ],
      "metadata": {
        "id": "5Q02P3SFJb34"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augment the text with TFIDF augmenter\n",
        "aug = naw.TfIdfAug(model_path='.', tokenizer=_tokenizer)\n",
        "augmented_text1 = aug.augment(text)\n",
        "print(f\"naw.TfIdfAug: {augmented_text1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf5lDN1EJzzY",
        "outputId": "26a7b9b6-92eb-4fb0-e450-24386ec4b992"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "naw.TfIdfAug: ['entitled signal column southwestern the upper deck does not Jimi well and YMA Herron replaced ZKJF4D_8UD new one']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Word Augmenter"
      ],
      "metadata": {
        "id": "KvxZLwDLKTIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "aug = naw.RandomWordAug()\n",
        "augmented_text2 = aug.augment(text)\n",
        "print(f\"naw.RandomWordAug: {augmented_text2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGeHtCfXJzth",
        "outputId": "51ba3f9a-0f97-421c-815f-b9de25e4500b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "naw.RandomWordAug: ['light signal column on the upper deck work well to new one']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstractive Summarization Augmenter"
      ],
      "metadata": {
        "id": "veWYj_fZfviN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.sentence as nas\n",
        "\n",
        "aug = nas.AbstSummAug()\n",
        "augmented_text3 = aug.augment(text)\n",
        "print(f\"nas.AbstSummAug: {augmented_text3}\")"
      ],
      "metadata": {
        "id": "ydi3dPLwKX06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c1f86c-4a7e-40d1-fc85-4b8f9b6bd46f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nas.AbstSummAug: ['light signal column on the upper deck does not work well and to be replaced with new one.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual Word Embeddings Augmenter"
      ],
      "metadata": {
        "id": "oIjgX-g-f6Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.sentence as nas\n",
        "\n",
        "# Contextual Word Embeddings - Sentence level\n",
        "aug = nas.ContextualWordEmbsForSentenceAug()\n",
        "augmented_text4 = aug.augment(text)\n",
        "print(f\"nas.ContextualWordEmbsForSentenceAug: {augmented_text4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHPQKQwAf1W7",
        "outputId": "c8a239f7-b9e5-47e3-dd09-fc809c83cc38"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nas.ContextualWordEmbsForSentenceAug: ['light signal column on the upper deck does not work well and to be replaced with new one to on on on a to and to , with : is at - ) is .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Augmenter"
      ],
      "metadata": {
        "id": "HKu1RTaegQwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.sentence as nas\n",
        "\n",
        "# Random Augmenter - Sentence level\n",
        "aug = nas.RandomSentAug()\n",
        "augmented_text5 = aug.augment(text)\n",
        "print(f\"nas.RandomSentAug: {augmented_text5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJygxSe-gIC0",
        "outputId": "008107fa-7ca7-44f4-ec24-850baca4fff8"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nas.RandomSentAug: ['light signal column on the upper deck does not work well and to be replaced with new one']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결과 모음"
      ],
      "metadata": {
        "id": "p8n1vAAsjCDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"원문: {text}\")\n",
        "print()\n",
        "\n",
        "print(f\"naw.TfIdfAug: {augmented_text1}\")\n",
        "print(f\"naw.RandomWordAug: {augmented_text2}\")\n",
        "print(f\"nas.AbstSummAug: {augmented_text3}\")\n",
        "print(f\"nas.ContextualWordEmbsForSentenceAug: {augmented_text4}\")\n",
        "print(f\"nas.RandomSentAug: {augmented_text5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHiwGkAbgUnb",
        "outputId": "25ce2ce6-111b-4d5e-abb0-4949ad326baa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문: light signal column on the upper deck does not work well and to be replaced with new one\n",
            "\n",
            "naw.TfIdfAug: ['entitled signal column southwestern the upper deck does not Jimi well and YMA Herron replaced ZKJF4D_8UD new one']\n",
            "naw.RandomWordAug: ['light signal column on the upper deck work well to new one']\n",
            "nas.AbstSummAug: ['light signal column on the upper deck does not work well and to be replaced with new one.']\n",
            "nas.ContextualWordEmbsForSentenceAug: ['light signal column on the upper deck does not work well and to be replaced with new one to on on on a to and to , with : is at - ) is .']\n",
            "nas.RandomSentAug: ['light signal column on the upper deck does not work well and to be replaced with new one']\n"
          ]
        }
      ]
    }
  ]
}